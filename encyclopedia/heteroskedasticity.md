# Heteroskedasticity

Given a [[linear-model]] such as $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u$, the model is homoskedastic (also spelled homoscedastic) if $Var(u | x_1, x_2) = \sigma^2$ is constant for all values of the explanatory variables $x_1$ and $x_2$, where $\sigma^2$ is the true variance of the dependent variable $y$ in the population. Intuitively this means that the variance of the error $u$ captures or explains the variance in the population, and that the precision (variance) of estimates for our dependent variable $y$ does not change for different values of our explanatory variables. If our model was heteroskedastic, there would be certain values of $x_1$ and $x_2$ for which the variance of $u$, and hence the variance of $y$ would be greater or smaller than others. Graphically, this would appear as a bulge or a pinch in a plot of the data or the residuals.

Heteroskedasticity does cause our estimators to become biased, but it does cause confidence intervals to become larger, resulting in less accurate hypothesis tests.

[[todo]] tests for this

[//begin]: # "Autogenerated link references for markdown compatibility"
[linear-model]: linear-model.md "linear-model"
[todo]: ../todo.md "Todo"
[//end]: # "Autogenerated link references"
